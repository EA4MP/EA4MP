learning rate: 1e-5
padding=True
truncation=True
max_length=512
batch_size = 8
epoch=8
Gradient Accumulation Steps=1
Weight Decay=0.01
adam_beta1=0.9
adam_beta2=0.999
adam_epsilon=1e-8
hidden_dropout_prob=0.1

for bert-base:
Hidden Layers: 12
Hidden size: 768
Attention heads: 12

for bert-large:
Hidden Layers: 24
Hidden size: 1024
Attention heads: 16


DT model:
criterion: gini
splitter: best
max_depth: None	
min_samples_split: 2	
min_samples_leaf: 1
max_features: None
max_leaf_nodes: None
min_impurity_decrease: 0.0
class_weight: None
random_state: 42	
ccp_alpha: 0.0	


NB-MODEL:
priors: None
var_smoothing: 1e-9
random_state: 42	



RF-MODEL:
n_estimators: 100	
criterion:	'gini'	
max_depth: 200
min_samples_split: 2	
min_samples_leaf: 1	
min_weight_fraction_leaf: 0.0	
max_features: 'auto'	
max_leaf_nodes: None	
min_impurity_decrease: 0.0	
bootstrap:	 True	
oob_score: False
n_jobs: None
random_state: 42
verbose: 0
warm_start: False	
class_weight: None	



svm-model
C: 1.0	
kernel: 'rbf'	
degree: 3	
gamma: 'rbf'	
coef0: 0.0	
shrinking: True	
probability: False	
tol: 1e-3	
cache_size: 200	
class_weight: None	
verbose: False	
max_iter: -1	
decision_function_shape: 'ovr'	
break_ties: False	
random_state: 42

